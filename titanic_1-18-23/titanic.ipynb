{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in data from csv file\n",
    "test = pd.read_csv(r'data\\test.csv')\n",
    "train = pd.read_csv(r'data\\train.csv')\n",
    "gender_submission = pd.read_csv(r'data\\gender_submission.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just a note\n",
    "I did this assingment a week ago before you had completed the notebook so this is my own personal notebook. still left comments in the same way you asked for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_id = test['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Cayden:\n",
    "# Drop the columns 'PassengerId', 'Name', 'Ticket', and 'Cabin' from the\n",
    "# training and test sets. The 'PassengerId' column is a unique identifier\n",
    "# number for each row, and it does not have any effect on the outcome.\n",
    "# The 'Name' and 'Ticket' columns are strings that do not have any effect\n",
    "# on the outcome. The 'Cabin' column has a lot of missing values, and it\n",
    "# also does not have any effect on the outcome.\n",
    "\n",
    "test = test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "train = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    418 non-null    int64  \n",
      " 1   Sex       418 non-null    object \n",
      " 2   Age       332 non-null    float64\n",
      " 3   SibSp     418 non-null    int64  \n",
      " 4   Parch     418 non-null    int64  \n",
      " 5   Fare      417 non-null    float64\n",
      " 6   Embarked  418 non-null    object \n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 23.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  891 non-null    int64  \n",
      " 1   Pclass    891 non-null    int64  \n",
      " 2   Sex       891 non-null    object \n",
      " 3   Age       714 non-null    float64\n",
      " 4   SibSp     891 non-null    int64  \n",
      " 5   Parch     891 non-null    int64  \n",
      " 6   Fare      891 non-null    float64\n",
      " 7   Embarked  889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 55.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cayde\\AppData\\Local\\Temp\\ipykernel_18716\\3357145229.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['IsAlone'].loc[train['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
      "C:\\Users\\cayde\\AppData\\Local\\Temp\\ipykernel_18716\\3357145229.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['IsAlone'].loc[test['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n"
     ]
    }
   ],
   "source": [
    "# 2 cayden: created 2 new features family size and is alone to hopefully improve the model. also filled in the missing values in the age and fare columns with the median.\n",
    "\n",
    "# Create a new feature FamilySize as a combination of SibSp and Parch\n",
    "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
    "test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n",
    "\n",
    "# Create new feature IsAlone from FamilySize\n",
    "train['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "train['IsAlone'].loc[train['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "test['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "test['IsAlone'].loc[test['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "\n",
    "# measure the correlation between family size and sibsp and parch and is alone\n",
    "correlation_df = train[['FamilySize', 'SibSp', 'Parch', 'IsAlone']].corr()\n",
    "\n",
    "# i removed the sibsp and parch columns because they are not very useful. \n",
    "# i also removed the is alone column because it is redundant with family size.\n",
    "\n",
    "test = test.drop(['SibSp', 'Parch', 'IsAlone'], axis=1)\n",
    "train = train.drop(['SibSp', 'Parch', 'IsAlone'], axis=1)\n",
    "\n",
    "# Fill in the missing values in the Age and Fare columns\n",
    "# In the training set, replace missing values with the median\n",
    "# In the test set, replace missing values with the median\n",
    "\n",
    "test['Age'] = test['Age'].fillna(test['Age'].median())\n",
    "test['Fare'] = test['Fare'].fillna(test['Fare'].median())\n",
    "\n",
    "train['Age'] = train['Age'].fillna(train['Age'].median())\n",
    "train['Fare'] = train['Fare'].fillna(train['Fare'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 cayden: 1 hot encoded all categorical columns to hopefully improve the model.\n",
    "\n",
    "# 1 hot encode the column 'Pclass'\n",
    "test = pd.get_dummies(test, columns=['Embarked'])\n",
    "test = pd.get_dummies(test, columns=['Sex'])\n",
    "\n",
    "train = pd.get_dummies(train, columns=['Embarked'])\n",
    "train = pd.get_dummies(train, columns=['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 cayden: standardize the age and fare columns to hopefully improve the model.\n",
    "\n",
    "# standardize the column age\n",
    "test['Age'] = (test['Age'] - test['Age'].mean()) / test['Age'].std()\n",
    "# standardize the column fare\n",
    "test['Fare'] = (test['Fare'] - test['Fare'].mean()) / test['Fare'].std()\n",
    "\n",
    "# do same standardization for train data\n",
    "train['Age'] = (train['Age'] - train['Age'].mean()) / train['Age'].std()\n",
    "train['Fare'] = (train['Fare'] - train['Fare'].mean()) / train['Fare'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Survived']\n",
    "X = train.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8667664670658682\n",
      "0.7847533632286996\n"
     ]
    }
   ],
   "source": [
    "# using scikit-learn to build a random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and validation data, for both features and target\n",
    "train, val, train_labels, val_labels = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# create the model\n",
    "model = RandomForestClassifier(n_estimators=400, max_depth=5, random_state=1)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train, train_labels)\n",
    "\n",
    "# make predictions for train data\n",
    "pred_train = model.predict(train)\n",
    "\n",
    "#get accuracy_score using sci-kit learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(train_labels, pred_train))\n",
    "\n",
    "# make predictions for validation data\n",
    "pred_val = model.predict(val)\n",
    "\n",
    "#get accuracy_score using sci-kit learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(val_labels, pred_val))\n",
    "\n",
    "# make predictions for test data\n",
    "pred_test = model.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 38.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "LogisticRegression                 0.80               0.79     0.79      0.80   \n",
      "LinearSVC                          0.79               0.78     0.78      0.79   \n",
      "CalibratedClassifierCV             0.79               0.78     0.78      0.79   \n",
      "RidgeClassifierCV                  0.79               0.78     0.78      0.79   \n",
      "RidgeClassifier                    0.79               0.78     0.78      0.79   \n",
      "LinearDiscriminantAnalysis         0.79               0.78     0.78      0.79   \n",
      "GaussianNB                         0.79               0.78     0.78      0.79   \n",
      "SGDClassifier                      0.79               0.78     0.78      0.79   \n",
      "NearestCentroid                    0.78               0.77     0.77      0.78   \n",
      "BernoulliNB                        0.78               0.77     0.77      0.78   \n",
      "SVC                                0.79               0.77     0.77      0.78   \n",
      "XGBClassifier                      0.78               0.77     0.77      0.78   \n",
      "NuSVC                              0.78               0.76     0.76      0.78   \n",
      "AdaBoostClassifier                 0.78               0.76     0.76      0.77   \n",
      "LGBMClassifier                     0.78               0.76     0.76      0.77   \n",
      "RandomForestClassifier             0.77               0.75     0.75      0.77   \n",
      "KNeighborsClassifier               0.77               0.75     0.75      0.76   \n",
      "ExtraTreesClassifier               0.74               0.73     0.73      0.74   \n",
      "BaggingClassifier                  0.75               0.73     0.73      0.74   \n",
      "DecisionTreeClassifier             0.73               0.71     0.71      0.73   \n",
      "LabelSpreading                     0.74               0.71     0.71      0.73   \n",
      "LabelPropagation                   0.74               0.71     0.71      0.73   \n",
      "ExtraTreeClassifier                0.72               0.70     0.70      0.71   \n",
      "Perceptron                         0.70               0.70     0.70      0.70   \n",
      "PassiveAggressiveClassifier        0.70               0.65     0.65      0.66   \n",
      "QuadraticDiscriminantAnalysis      0.59               0.52     0.52      0.45   \n",
      "DummyClassifier                    0.57               0.50     0.50      0.42   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "LogisticRegression                   0.01  \n",
      "LinearSVC                            0.02  \n",
      "CalibratedClassifierCV               0.08  \n",
      "RidgeClassifierCV                    0.01  \n",
      "RidgeClassifier                      0.01  \n",
      "LinearDiscriminantAnalysis           0.01  \n",
      "GaussianNB                           0.01  \n",
      "SGDClassifier                        0.01  \n",
      "NearestCentroid                      0.01  \n",
      "BernoulliNB                          0.01  \n",
      "SVC                                  0.02  \n",
      "XGBClassifier                        0.05  \n",
      "NuSVC                                0.02  \n",
      "AdaBoostClassifier                   0.07  \n",
      "LGBMClassifier                       0.05  \n",
      "RandomForestClassifier               0.14  \n",
      "KNeighborsClassifier                 0.01  \n",
      "ExtraTreesClassifier                 0.12  \n",
      "BaggingClassifier                    0.02  \n",
      "DecisionTreeClassifier               0.01  \n",
      "LabelSpreading                       0.02  \n",
      "LabelPropagation                     0.02  \n",
      "ExtraTreeClassifier                  0.01  \n",
      "Perceptron                           0.01  \n",
      "PassiveAggressiveClassifier          0.01  \n",
      "QuadraticDiscriminantAnalysis        0.01  \n",
      "DummyClassifier                      0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 cayden:  i used lazy predict to find the best model. then i used best model from lazypred( logistic regression) to make predictions for test data. (i also tried using the random forest model from scikit learn but it did not perform as well as the logistic regression model\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "# convert the data to numpy arrays\n",
    "train = train.to_numpy()\n",
    "val = val.to_numpy()\n",
    "train_labels = train_labels.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n",
    "\n",
    "#using train, val, train_labels, val_labels use lazy predict to find the best model\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(train, val, train_labels, val_labels)\n",
    "print(models)\n",
    "\n",
    "# use LogisticRegression model to make predictions for test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "model = LogisticRegression(max_iter=1000, random_state= 1)\n",
    "\n",
    "# split the data into training and validation data, for both features and target\n",
    "train, val, train_labels, val_labels = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train, train_labels)\n",
    "\n",
    "# make predictions for train data\n",
    "pred_train = model.predict(train)\n",
    "\n",
    "#get accuracy_score using sci-kit learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(train_labels, pred_train))\n",
    "\n",
    "# make predictions for validation data\n",
    "pred_val = model.predict(val)\n",
    "\n",
    "#get accuracy_score using sci-kit learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(val_labels, pred_val))\n",
    "\n",
    "# make predictions for test data\n",
    "pred_test = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a submission dataframe\n",
    "submission = pd.DataFrame({'PassengerId': passenger_id, 'Survived': pred_test})\n",
    "# convert the dataframe to a csv file that can be uploaded\n",
    "# this is saved in the same directory as your notebook\n",
    "filename = 'titanic-predictions.csv'\n",
    "\n",
    "submission.to_csv(filename, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sub1_clas__sub2_rf](kaggle_score.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1ebc6ce21e20a44ca2e939d468f8c4914f49f277b8be916293753b9e7a9feef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
